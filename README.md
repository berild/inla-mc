# Importance Sampling with the Integrated Nested Laplace Approximation

This repository contains the code used in the Importance Sampling with the Integrated Nested Laplace Approximation paper. The implementation of the Importance Sampling with the Integrated Nested Laplace Approximation (IS-INLA) can be found in <a href="https://github.com/berild/inla-mc/blob/master/inlaIS.R">inlaIS.R</a>, the Adaptive Multiple Importance Sampling with the Integrated Nested Laplace Approximation (AMIS-INLA) in <a href="https://github.com/berild/inla-mc/blob/master/inlaAMIS.R">inlaAMIS.R</a>, and the Markov Chain Monte Carlo with the Integrated Nested Laplace Approximation (MCMC-INLA) in <a href="https://github.com/berild/inla-mc/blob/master/inlaMH.R">inlaMH.R</a>. In addition, general functions used in all algorithms are collected in <a href="https://github.com/berild/inla-mc/blob/master/genFuncs.R">genFuncs.R</a>. 


## Usage

To fit a conditional latent Gaussian model with our implementation of the INLA within Monte Carlo methods, a specific function call is required in the respective methods. This call includes the data (`data`), and initial parameters of the proposal distribution (`init = list(x = ... , sigma = ...)`) which could be a mean and covariance matrix but can be anything. The methods also requires some functions as input. The first is the prior (`prior()`) of the parameters being conditioned on $z_c$
```r
prior <- function(x, log = TRUE){
  return(sum(dnorm(x, mean = 0, sd= sqrt(1/.001), log = log)))
}
```
where `log` needs to be a input such that the function can return the log-probability if specified (for in MCMC). Next it requires the proposal distribution
```r
dprop <- function(y, x, sigma = diag(5,2,2), log = TRUE){
  return(mvtnorm::dmvnorm(y, mean = x, sigma = sigma, log = log))
}
```
for evaluation, where `x` is the `init$x` parameter and `sigma` the `init$sigma` parameter, and `log` also needs to be a input here. Furthermore it requires the same proposal distribution
```r
rprop <- function(x, sigma = diag(5,2,2)){
  return(mvtnorm::rmvnorm(1, mean=x, sigma = sigma))
}
```
for sampling. Here, we have used the multivariate Gaussian proposal distribution but any proposal distribution can be used with any parameters `init$x` and `init$sigma`. In MCMC-INLA, the `init` parameters only specifies the initial state, and will later use the previous state as input `x`. 
Lastly, a function that uses INLA to fit the conditional model given $z_c$ is required. 

```r
fit.inla <- function(data, beta){
  data$oset = data$x %*% t(beta)
  res = inla(y ~ 1 + offset(oset), data = data)
  return(list(mlik = res$mlik,
               dists = list(intercept = res$marginals.fixed[[1]],
                            precision = res$marginals.hyperpar[[1]])))
}
```
Here, the `beta` parameter represents $z_c$. This function needs to return the conditional posterior marginals of $z_{-c}$ in a list (`dists`) and the conditional marginal likelihood (`mlik`). These inputs are common for all methods (IS-INLA, AMIS-INLA, MCMC-INLA), but there are also specific inputs that controls the respective methods. 

To use the IS-INLA method to fit your conditional LGM simply run the following function:
```r
res = inlaIS(data, init, prior, dprop, rprop, fit.inla, N_0, N, ncores)
```
with the inputs described above. Additionally, the input `N_0` specifies the number of samples used in a initial search for a better proposal distribution, `N` is the number of samples of $z_c$ used in the resulting approximation , and `ncores` is the number of CPU cores used.

The AMIS-INLA method is very similar to the IS-INLA function call:
```r
res = inlaAMIS(data, init, prior, dprop, rprop, fit.inla, N_t, N_0, ncores)
```
Here, `N_t` is a vector e.q. `seq(250,500,10)` specifying the number of samples generated by each proposal distribution or prior to each adaptation, and `N_0` is the initial number of samples. 

Lastly, to run our implementation of the MCMC-INLA algorithm simply run the following
```r
res = inlaMH(data, init, prior, dprop, rprop, fit.inla, n.samples, n.burnin, n.thin)
```
where `n.samples` is the number of generated samples, `n.burnin` is the number of samples in the burn-in, and `n.thin` is the number samples in the thinning of the Markov Chain. A implementation of this algorithm can also be found in the `INLABMA::INLAMH()`.
