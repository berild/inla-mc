---
title: "Checks and convergence assessment for IS with INLA"
#author: "..."
date: "9/2/2021" 
output: html_document
---

# Introduction

This is to test the distribution of the weights obtained with importance
sampling under different scenarios. The aim is to obtain some empirical
evidence about how *good* weights should be. 

The aim is to estimate the mean of a bivariate normal distribution. 
The model is made of a bivariate normal distribution with mean $\mu_1 = (0, 0)^{\top}$ and
covariance matrix

$$
\Sigma_1 = 
\left[
\begin{array}{cc}
1 & \rho\\
\rho & 1\\
\end{array}
\right]
$$
with $\rho$ a correlation parameter.

First of all some general parameters are set to conduct the simulation study:


```{r}
# Set RNG seed
set.seed(1)

# Number of observed data
n.data <- 100

# Number of samples for IS
n.samples <- 10000
```

The parameters of the data generating distribution are defined below.

```{r}
# Likelihood
mu1 <- c(0, 0)
rho <- 0.1
Sigma1 <- matrix(c(1, rho, rho, 1), ncol = 2)
```


Next, `n.data` observations from previous multivariate normal distribution
are sampled:

```{r}
library("mvtnorm")
# Observed data
y <- rmvnorm(n.data, mean = mu1, sigma = Sigma1)
```

The following function will compute the log-likelihood for a given value of
the mean and precision (which is supposed to be known):

```{r}
# y: two-column matrix with observed data
# mu: vector of means
# sigma: COvariance matrix
log_lik <- function(y, mu, sigma = Sigma1) {
   return(sum(dmvnorm(y, mu, sigma, log = TRUE)))
}
```

The priors on the means (i.e., the elements of $\mu_1$) are two independent normals with zero mean and
variance 1000. This function will compute the prior:

```{r}
# mu: Values of the mean vector
log_prior <- function(mu) {
  return(sum(dnorm(mu, mean = 0, sd = sqrt(1000))))
} 
```


The sampling distribution will be a bivariate normal with different precision
matrices to mimick exact, good and bad sampling distributions.

```{r}
# Sampling distribution
mu2 <- c(0, 0)
rho2 <- 0
Sigma2 <-matrix(c(1, rho2, rho2, 1), ncol = 2) 
```

## Posterior distribution

The actual posterior distribution of $\mu_1$ is a bivariate normal, with posterior mean as follows:

$$
(n\Sigma_1^{-1} + \Sigma^{-1}_0)^{-1} (\Sigma^{-1}_0 \mu_0 + \Sigma_1^{-1} \sum y)
$$


The posterior covariance is:

$$
(n\Sigma_1^{-1} + \Sigma^{-1}_0)^{-1}
$$
with $\Sigma_0$ the covariance matrix of the prior (which is an identity matrix).

```{r}
Sigma0 <- matrix(c(1, 0, 0, 1), ncol = 2)
```

The posterior covariance is computed first:

```{r}
post.covar <- solve((n.data * solve(Sigma1) + solve(Sigma0)))
post.covar
```

Then, using the posterior covariance, the posterior mean can be computed:

```{r}
post.mean <- post.covar %*% (solve(Sigma0) %*% matrix(c(0, 0), ncol = 1) +
  solve(Sigma1) %*% matrix(apply(y, 2, sum), ncol = 1))
post.mean
```


## Importance Sampling


In this section three scenarios will be explored. The firts one is sampling from
the exact posterior. This will yield very similar weights. The second scenario considers a very wide sampling distribution, while the last scenario uses a very narrow sampling distribution. The second scenario should be able to estimate the
posterior distribution, but very inefficiently (i.e., very small effective sample size). Finally, the third scenario may struggle to estimate the posterior distribution. All the sampling distribution are centered at the actual value of 
$mu_1$.

```{r}
# Samples (proposal)
#smps <- rmvnorm(n.samples, mean = mu2, sigma = Sigma2)


# Function to compute the weights
# smps: Samples, as two-column matrix
compute_weights <- function(smps) {

  # Log-Weights
  log_w <- apply(smps, 1, function(X) {
    log_aux <- log_lik(y, X) + log_prior(X) - dmvnorm(X, mean = mu2, sigma = Sigma2)
    return(log_aux)
  })

  # Summary log-weights
  #summary(log_w)

  # Re-scale weights
  ww <- exp(log_w - max(log_w))
  ww <- ww / sum(ww)

  # Summary weights
  #summary(ww)

  #Means
  est.means <- apply(smps, 2, function(X) { sum(X * ww)})
  est.means

  # Variances
  est.vars <- apply(smps, 2, function(X) { sum(X^2*ww) - sum(X * ww)^2})
  est.vars

  #Covariance
  est.cov <- sum(ww * apply(smps, 1, function(X) {
    (X[1] - est.means[1]) * (X[2] - est.means[2])
  }))
  est.cov

  # Sample size (to estimate the mean)
  n_e <- (sum(ww)^2) / sum(ww^2)

  # Sample size (to estimate the variance variance)
  n_e_sigma <- (sum(ww^2)^2)/(sum(ww^4))

  # Sample size (to estimate the kurtosis)
  # n_e_gamma
  n_e_gamma <- (sum(ww^2)^3) / (sum(ww^3)^2)

  return(list(weights = ww, mean = est.means,
    cov = matrix(c(est.vars[1], est.cov, est.cov, est.vars[2]), ncol = 2),
    n_e = n_e, n_e_sigma = n_e_sigma, n_e_gamma = n_e_gamma))
}


# Print results
print_results <- function(res) {
  cat("\nPosterior means:\n")
  print(res$mean)
  cat("\nPosterior covariance:\n")
  print(res$cov)
  cat("\nSample size (to estimate the mean)\n")
  print(res$n_e)
  cat("\nSample size (to estimate the variance)\n")
  print(res$n_e_sigma)
  cat("\nSample size (to estimate the kurtosis)\n")
  print(res$n_e_gamma)
}
```

```{r}
# Samples: true posterior 
smps1 <- rmvnorm(n.samples, mean = post.mean, sigma = post.covar)
res1 <- compute_weights(smps1)
print_results(res1)
```


```{r fig = TRUE, echo = FALSE}
par(mfrow = c(1, 2))
hist(log(res1$weights), main = "Log-weights")
hist(res1$weights, main = "Weights")
```

### Sampling from wide proposal

```{r}
# Samples: true posterior 
Sigma_wide <- matrix(c(10, 0, 0, 10), ncol = 2)
smps2 <- rmvnorm(n.samples, mean = c(0, 0), sigma = Sigma_wide)
res2 <- compute_weights(smps2)
print_results(res2)
```


```{r fig = TRUE, echo = FALSE}
par(mfrow = c(1, 2))
hist(log(res2$weights), main = "Log-weights")
hist(res2$weights, main = "Weights")
```



### Sampling from narrow proposal


The *narrow* proposal is made by using a diagonal covriance matrix with
entires 1/10 in the diagonal.

```{r}
# Samples: true posterior 
Sigma_narrow <- matrix(c(1 / 10, 0, 0, 1 / 10), ncol = 2)
smps3 <- rmvnorm(n.samples, mean = c(0, 0), sigma = Sigma_narrow)
res3 <- compute_weights(smps3)
print_results(res3)
```


```{r fig = TRUE, echo = FALSE}
par(mfrow = c(1, 2))
hist(log(res3$weights), main = "Log-weights")
hist(res3$weights, main = "Weights")
```



